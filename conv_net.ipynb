{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training convolutional networks\n",
    "\n",
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the pickle file\n",
    "\n",
    "pickle_file = '/Users/rgparekh/Documents/Personal/Rajesh/Data/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Re-shape the data sets to be TensorFlow friendly\n",
    "\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define accuracy\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the CNN architecture\n",
    "\n",
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    # data is the input data\n",
    "    # layer1_weights define the filter parameter [filter_height, filter_width, input_channels, output_channels]\n",
    "    # [1,2,2,1] defines the strides parameter - 1D tensor of length 4 (batch, height, width, channels)\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.AdamOptimizer(0.005).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.927531\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 50: 1.046034\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 69.7%\n",
      "Minibatch loss at step 100: 0.901807\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 150: 0.285646\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 200: 0.875015\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.0%\n",
      "Minibatch loss at step 250: 0.912396\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 300: 0.293089\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 350: 0.437883\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 400: 0.215000\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 450: 0.868602\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 500: 0.505556\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 550: 0.848638\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 600: 0.359397\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 650: 0.807611\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 700: 0.698258\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 750: 0.066098\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 800: 0.693908\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 850: 1.205892\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 900: 0.443825\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 950: 0.403484\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 1000: 0.251561\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.6%\n",
      "Test accuracy: 90.8%\n"
     ]
    }
   ],
   "source": [
    "# Train model and compute test set accuracy\n",
    "\n",
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convolutional Neural Network from the TensorFlow tutorial: https://www.tensorflow.org/tutorials/layers\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the pickle file\n",
    "\n",
    "pickle_file = '/Users/rgparekh/Documents/Personal/Rajesh/Data/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels [0] 4\n",
      "Test labels [0] 3\n",
      "Valid labels [0] 1\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "print('Train labels [0]', train_labels[0])\n",
    "print('Test labels [0]', test_labels[0])\n",
    "print('Valid labels [0]', valid_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Re-shape the data sets to be TensorFlow friendly\n",
    "\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels [0] [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "Test labels [0] [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "Valid labels [0] [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "# Labels are now in 1-hot encoding\n",
    "print('Train labels [0]', train_labels[0])\n",
    "print('Test labels [0]', test_labels[0])\n",
    "print('Valid labels [0]', valid_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/notmnist_convnet_model/model.ckpt-2400\n",
      "INFO:tensorflow:Saving checkpoints for 2401 into /tmp/notmnist_convnet_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.16968, step = 2401\n",
      "INFO:tensorflow:probabilities = [[ 0.19444923  0.01019515  0.00028452  0.00619345  0.00154695  0.00160887\n",
      "   0.0022322   0.00974129  0.08458734  0.68916106]\n",
      " [ 0.05345789  0.0337581   0.00740292  0.01530634  0.06090756  0.03013228\n",
      "   0.00908712  0.0134669   0.5755437   0.20093717]\n",
      " [ 0.98760825  0.00187131  0.00022567  0.00087676  0.00046516  0.00033656\n",
      "   0.00105335  0.0016172   0.00518806  0.00075763]\n",
      " [ 0.00507217  0.0433133   0.03007456  0.80659485  0.00592063  0.00383412\n",
      "   0.07795572  0.01451581  0.00069871  0.01202016]\n",
      " [ 0.27241713  0.05756799  0.04268062  0.02204186  0.09196025  0.10008361\n",
      "   0.10529311  0.11089665  0.04250158  0.15455714]\n",
      " [ 0.07227923  0.25870728  0.03735801  0.10863309  0.06833531  0.10607515\n",
      "   0.0360863   0.13650978  0.09128364  0.08473219]\n",
      " [ 0.25380793  0.18325451  0.0098953   0.08479972  0.02058626  0.0829994\n",
      "   0.02188426  0.07592     0.124693    0.1421596 ]\n",
      " [ 0.00026087  0.00071848  0.00089699  0.00021648  0.02861757  0.96658105\n",
      "   0.00095671  0.00100709  0.00072755  0.00001721]\n",
      " [ 0.03743822  0.17630263  0.00559394  0.00704282  0.0467314   0.01850938\n",
      "   0.00767841  0.03689416  0.57093817  0.09287095]\n",
      " [ 0.51395684  0.07390771  0.05282701  0.01009175  0.05799334  0.03630426\n",
      "   0.04667115  0.12998471  0.03677713  0.04148619]\n",
      " [ 0.00027659  0.02183023  0.17586429  0.55456102  0.00547417  0.00015316\n",
      "   0.23618619  0.00084745  0.00314418  0.00166276]\n",
      " [ 0.01485913  0.02599582  0.04291075  0.00369286  0.23096859  0.59122616\n",
      "   0.02046752  0.06394366  0.00455836  0.00137713]\n",
      " [ 0.00396005  0.00183664  0.0005512   0.00083151  0.00163766  0.00031813\n",
      "   0.00311716  0.00067401  0.1599301   0.82714349]\n",
      " [ 0.09834184  0.15874615  0.02743007  0.15032612  0.06694092  0.0062352\n",
      "   0.05537023  0.20726569  0.05755441  0.17178938]\n",
      " [ 0.16185659  0.25975576  0.00336624  0.0092429   0.02561324  0.00729825\n",
      "   0.0684908   0.34375006  0.0554232   0.06520299]\n",
      " [ 0.01341681  0.0470752   0.2333291   0.03863277  0.05052471  0.00576445\n",
      "   0.58424443  0.01705485  0.00285824  0.00709946]\n",
      " [ 0.04540389  0.21258643  0.01385826  0.46037555  0.00427204  0.02209648\n",
      "   0.03230174  0.06518741  0.05959824  0.08431987]\n",
      " [ 0.08299512  0.00980853  0.0014999   0.01128488  0.00129818  0.00049965\n",
      "   0.01540814  0.00299616  0.01050541  0.86370403]\n",
      " [ 0.00713808  0.1384324   0.00113222  0.82498211  0.00588753  0.00270862\n",
      "   0.00399329  0.00898118  0.00131236  0.00543214]\n",
      " [ 0.00227405  0.03217562  0.25117961  0.01084408  0.40714678  0.17164069\n",
      "   0.09297477  0.01783635  0.01104123  0.0028868 ]\n",
      " [ 0.00750873  0.01586162  0.00229733  0.00265156  0.14438131  0.80679387\n",
      "   0.00218221  0.01728486  0.00069068  0.00034773]\n",
      " [ 0.00203391  0.0098498   0.19126707  0.01273874  0.01373444  0.00167642\n",
      "   0.7664876   0.00167385  0.00037906  0.00015916]\n",
      " [ 0.00771006  0.02349701  0.26489797  0.0139759   0.30667081  0.00837128\n",
      "   0.33159173  0.01542191  0.01843726  0.00942612]\n",
      " [ 0.99024606  0.00203163  0.0000294   0.00036588  0.00040134  0.0002566\n",
      "   0.0009192   0.00503867  0.00037549  0.00033572]\n",
      " [ 0.06380761  0.10046138  0.00871622  0.01670144  0.02016051  0.00437713\n",
      "   0.14383581  0.00754188  0.62910968  0.0052883 ]\n",
      " [ 0.0832162   0.05435103  0.05623514  0.13858114  0.03144591  0.0729858\n",
      "   0.17533858  0.07439014  0.16562648  0.14782959]\n",
      " [ 0.00526854  0.87649155  0.00644982  0.02870348  0.01529794  0.00042169\n",
      "   0.0092596   0.04972209  0.00646115  0.0019241 ]\n",
      " [ 0.00985546  0.89410847  0.00156596  0.034155    0.01001849  0.00500088\n",
      "   0.00961517  0.03150026  0.00206482  0.00211533]\n",
      " [ 0.06065306  0.5106141   0.01417558  0.03612237  0.04721343  0.03406305\n",
      "   0.04504183  0.04563489  0.18454681  0.02193498]\n",
      " [ 0.00803127  0.01756518  0.01020061  0.80345571  0.0039117   0.0014092\n",
      "   0.01356742  0.13669989  0.00053292  0.00462611]\n",
      " [ 0.01678526  0.20848186  0.08402403  0.00758629  0.08453608  0.23172505\n",
      "   0.01596875  0.2030215   0.08651739  0.06135379]\n",
      " [ 0.88329172  0.01297614  0.00017505  0.00028052  0.00209423  0.00034191\n",
      "   0.00240898  0.0190821   0.01038058  0.06896876]\n",
      " [ 0.86295217  0.08436895  0.00049874  0.00282159  0.00290555  0.00109532\n",
      "   0.01002171  0.02686844  0.00592987  0.00253774]\n",
      " [ 0.06363628  0.0959973   0.0166913   0.04897076  0.0677642   0.00536166\n",
      "   0.00686249  0.09944969  0.58943349  0.00583282]\n",
      " [ 0.13557798  0.09795131  0.04360891  0.02786167  0.04228831  0.16458595\n",
      "   0.05784836  0.14084624  0.11167565  0.17775561]\n",
      " [ 0.00326023  0.04116419  0.03096182  0.00460784  0.76172692  0.11889586\n",
      "   0.00862361  0.00721271  0.02307486  0.00047199]\n",
      " [ 0.00301194  0.01126262  0.00248785  0.0004379   0.01275557  0.00770556\n",
      "   0.00139145  0.0005099   0.94032949  0.02010771]\n",
      " [ 0.00527112  0.18216933  0.00059757  0.01642245  0.00449757  0.00370618\n",
      "   0.0011679   0.78524292  0.00076557  0.00015944]\n",
      " [ 0.05561299  0.06874667  0.00497239  0.04630972  0.0056434   0.00711082\n",
      "   0.06495298  0.00625625  0.47315872  0.26723605]\n",
      " [ 0.00011379  0.00061012  0.78433198  0.00189033  0.17766647  0.01014789\n",
      "   0.02449577  0.00023129  0.00048499  0.00002744]\n",
      " [ 0.1322334   0.24314174  0.00595445  0.01185486  0.03927548  0.00825235\n",
      "   0.06318465  0.08608966  0.38375965  0.02625383]\n",
      " [ 0.01368666  0.01289755  0.04117633  0.03590561  0.00761534  0.000584\n",
      "   0.87887031  0.00732589  0.00045298  0.00148527]\n",
      " [ 0.01232213  0.1570918   0.00023729  0.015514    0.01709644  0.03344772\n",
      "   0.00594237  0.75642663  0.00090199  0.00101961]\n",
      " [ 0.00273232  0.06615812  0.01516679  0.74652135  0.00807545  0.00165887\n",
      "   0.12254123  0.03043901  0.00126848  0.00543837]\n",
      " [ 0.00885755  0.66083837  0.00118116  0.03021242  0.10084899  0.01382931\n",
      "   0.00889594  0.17038804  0.00103376  0.00391448]\n",
      " [ 0.01940583  0.36834681  0.02999191  0.05234706  0.24464171  0.00582512\n",
      "   0.22601506  0.03142369  0.02053584  0.0014669 ]\n",
      " [ 0.03196676  0.40798849  0.00167726  0.34394562  0.06679586  0.00260092\n",
      "   0.02715715  0.10287169  0.01040927  0.00458707]\n",
      " [ 0.02296225  0.08015276  0.02981329  0.08266905  0.02194162  0.01061158\n",
      "   0.12556787  0.04262753  0.56880844  0.01484556]\n",
      " [ 0.0504854   0.00545275  0.00015192  0.00865056  0.00022113  0.00004845\n",
      "   0.00537162  0.00110106  0.01806848  0.91044855]\n",
      " [ 0.01894783  0.15414616  0.00058775  0.01213114  0.01172006  0.01223971\n",
      "   0.01614626  0.76663685  0.00602151  0.00142267]\n",
      " [ 0.04543333  0.2389614   0.00152303  0.04028898  0.00733718  0.00023776\n",
      "   0.01172645  0.5363974   0.05067389  0.06742059]\n",
      " [ 0.00398076  0.03492102  0.03870156  0.02228992  0.63841921  0.19635826\n",
      "   0.02188166  0.0331251   0.00875521  0.0015673 ]\n",
      " [ 0.00497669  0.00168207  0.00397473  0.00156817  0.03585574  0.05093387\n",
      "   0.00402502  0.00054859  0.85226226  0.04417276]\n",
      " [ 0.00116308  0.01600528  0.06772702  0.00225509  0.7814945   0.10026643\n",
      "   0.0195829   0.00243429  0.0087258   0.00034559]\n",
      " [ 0.00006093  0.00146633  0.85608017  0.01040746  0.06793211  0.00509029\n",
      "   0.05823017  0.00019536  0.00039578  0.00014143]\n",
      " [ 0.00437446  0.09851395  0.43169355  0.05058902  0.1752833   0.07941299\n",
      "   0.04180986  0.09032015  0.00919698  0.01880582]\n",
      " [ 0.05700478  0.19732401  0.08799574  0.30121249  0.16104339  0.00280743\n",
      "   0.03542453  0.02059544  0.09879003  0.0378022 ]\n",
      " [ 0.04866643  0.11332656  0.06183264  0.21360652  0.11952183  0.09504223\n",
      "   0.07223126  0.01737156  0.23999223  0.01840877]\n",
      " [ 0.00355173  0.00747633  0.00004717  0.00765685  0.00036419  0.00000768\n",
      "   0.00296431  0.00335489  0.00488101  0.96969587]\n",
      " [ 0.00030739  0.00081242  0.6911661   0.01515636  0.09806666  0.15957962\n",
      "   0.03241529  0.00010315  0.00231887  0.00007413]\n",
      " [ 0.01418423  0.00457251  0.00081501  0.0080717   0.0006446   0.00013432\n",
      "   0.00173262  0.00527772  0.01173776  0.95282948]\n",
      " [ 0.05716567  0.05557074  0.11525434  0.26035514  0.20597516  0.06333321\n",
      "   0.08936854  0.03548212  0.03520247  0.08229269]\n",
      " [ 0.80749798  0.01352728  0.00808837  0.00295086  0.01617594  0.01143249\n",
      "   0.04102825  0.06482404  0.00891172  0.02556313]\n",
      " [ 0.00589315  0.01837075  0.01918678  0.00136533  0.75043553  0.1877241\n",
      "   0.00622689  0.00752285  0.0029178   0.00035679]\n",
      " [ 0.01843737  0.01374645  0.00701212  0.00165958  0.08987379  0.007841\n",
      "   0.00267784  0.00144391  0.82418698  0.033121  ]\n",
      " [ 0.04176651  0.13024834  0.00261485  0.04136751  0.01257599  0.00736876\n",
      "   0.03678831  0.71891356  0.00194634  0.0064098 ]\n",
      " [ 0.00018289  0.00105266  0.00710598  0.00014413  0.08116587  0.90613872\n",
      "   0.00133929  0.00239532  0.00046211  0.00001308]\n",
      " [ 0.0000223   0.0002342   0.93413311  0.00071996  0.03453123  0.00166344\n",
      "   0.02844246  0.00002008  0.00022319  0.00000995]\n",
      " [ 0.00466028  0.00248761  0.00000364  0.00234906  0.00002591  0.00001007\n",
      "   0.00017039  0.00041226  0.00429662  0.98558414]\n",
      " [ 0.03220898  0.73404789  0.00348005  0.05777129  0.00491442  0.00123238\n",
      "   0.01061826  0.14809255  0.00241534  0.00521892]\n",
      " [ 0.00225306  0.0029742   0.00002632  0.00114111  0.00002968  0.0000019\n",
      "   0.00026647  0.0014895   0.00057423  0.99124348]\n",
      " [ 0.0048298   0.00218396  0.00010864  0.00303929  0.0000557   0.00001693\n",
      "   0.00193126  0.00307586  0.00189225  0.98286635]\n",
      " [ 0.08876837  0.05176841  0.06116574  0.34854355  0.04023968  0.02391629\n",
      "   0.16653246  0.04591938  0.01616871  0.15697742]\n",
      " [ 0.0047199   0.00251561  0.00001365  0.00059436  0.00000539  0.00000259\n",
      "   0.00107179  0.00106548  0.00059232  0.98941886]\n",
      " [ 0.01663109  0.3285909   0.0217061   0.38195339  0.01469018  0.00215468\n",
      "   0.18919583  0.02675543  0.00395338  0.01436895]\n",
      " [ 0.00815234  0.11425853  0.04685632  0.68294865  0.00147574  0.00504389\n",
      "   0.0622424   0.07263211  0.00089977  0.00549021]\n",
      " [ 0.02035589  0.21771383  0.0024509   0.01839937  0.00830486  0.00223518\n",
      "   0.01556338  0.70841962  0.0004555   0.00610146]\n",
      " [ 0.03255967  0.22683565  0.02439564  0.04792775  0.05946694  0.01156326\n",
      "   0.02295277  0.0167534   0.47656858  0.08097633]\n",
      " [ 0.02715434  0.18658787  0.10232119  0.10636598  0.24199714  0.07754877\n",
      "   0.12660265  0.05770222  0.03081108  0.04290884]\n",
      " [ 0.83063483  0.01596828  0.00408653  0.005782    0.02142146  0.01186775\n",
      "   0.05132616  0.04775716  0.00612703  0.00502873]\n",
      " [ 0.19101439  0.18374085  0.00046416  0.02203277  0.00059845  0.00004546\n",
      "   0.02412179  0.19310983  0.00584945  0.37902284]\n",
      " [ 0.00895966  0.1346958   0.00034756  0.00961334  0.0009916   0.00449773\n",
      "   0.00458578  0.83447737  0.00060643  0.00122473]\n",
      " [ 0.00272885  0.01328085  0.01237336  0.00974435  0.17272057  0.7518428\n",
      "   0.0021219   0.03201979  0.00287364  0.00029387]\n",
      " [ 0.9930737   0.00205874  0.00006698  0.00035914  0.00037211  0.00005241\n",
      "   0.00042124  0.00088504  0.00086526  0.00184537]\n",
      " [ 0.09332611  0.06802382  0.02823871  0.08357645  0.05092092  0.24539137\n",
      "   0.03599905  0.034584    0.07128508  0.28865445]\n",
      " [ 0.52990508  0.10029828  0.00885914  0.13217521  0.01603735  0.00256223\n",
      "   0.05598513  0.02395737  0.0785903   0.05162996]\n",
      " [ 0.01288322  0.02529711  0.0355613   0.02888406  0.19264439  0.64235228\n",
      "   0.01156321  0.01695079  0.03183254  0.00203107]\n",
      " [ 0.13846995  0.63230187  0.00642856  0.0572831   0.0412083   0.0146733\n",
      "   0.01722195  0.08083137  0.004249    0.00733262]\n",
      " [ 0.05227182  0.08126201  0.00208099  0.0041893   0.01226295  0.12256414\n",
      "   0.009941    0.68860734  0.01007042  0.01675007]\n",
      " [ 0.04268315  0.0421364   0.00530337  0.0198388   0.03027994  0.0108779\n",
      "   0.02570777  0.80057788  0.00550694  0.0170878 ]\n",
      " [ 0.01957354  0.02743641  0.16783695  0.12905137  0.08345041  0.00558135\n",
      "   0.49527505  0.00410094  0.06525028  0.00244362]\n",
      " [ 0.01379339  0.2172533   0.0130894   0.67842603  0.01170448  0.00679977\n",
      "   0.02882351  0.01432663  0.00947292  0.00631057]\n",
      " [ 0.09022326  0.01098766  0.00434151  0.00313708  0.0089615   0.08714138\n",
      "   0.0066501   0.0029037   0.43036905  0.35528475]\n",
      " [ 0.00715033  0.04269005  0.12828209  0.04787606  0.00992065  0.0023192\n",
      "   0.74363744  0.00110922  0.00425845  0.01275659]\n",
      " [ 0.00064703  0.00083345  0.65733278  0.02422244  0.09227118  0.07943764\n",
      "   0.14231089  0.0017453   0.00069988  0.00049939]\n",
      " [ 0.00371567  0.00364619  0.00472111  0.00088234  0.01933225  0.02229369\n",
      "   0.0006292   0.00112684  0.90099764  0.04265505]\n",
      " [ 0.06465556  0.07103955  0.13109528  0.11379472  0.17544776  0.12738611\n",
      "   0.08193821  0.09346583  0.05289964  0.08827733]\n",
      " [ 0.1140156   0.2844021   0.03020149  0.12398759  0.03982301  0.01460446\n",
      "   0.13667618  0.21290387  0.03132856  0.01205714]\n",
      " [ 0.01449506  0.01242242  0.09600471  0.00797437  0.21815875  0.62008077\n",
      "   0.00875018  0.02029818  0.00151223  0.00030341]\n",
      " [ 0.20443286  0.03833464  0.03620902  0.01393347  0.01708459  0.00200392\n",
      "   0.07325673  0.03082111  0.46114522  0.12277848]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 4.6453\n",
      "INFO:tensorflow:loss = 1.10964, step = 2501 (21.528 sec)\n",
      "INFO:tensorflow:probabilities = [[ 0.00434023  0.00094209  0.00448475  0.00082615  0.01971925  0.96312159\n",
      "   0.00116955  0.00511949  0.0002286   0.00004839]\n",
      " [ 0.17727886  0.07474162  0.00025283  0.00369421  0.01349015  0.038057\n",
      "   0.01169052  0.64046884  0.01259759  0.02772831]\n",
      " [ 0.06568386  0.04632153  0.24507834  0.04434178  0.04869646  0.01313833\n",
      "   0.4402324   0.03593935  0.01618609  0.04438193]\n",
      " [ 0.10416142  0.07112595  0.01446772  0.6917088   0.0105937   0.01207648\n",
      "   0.03944845  0.0258574   0.00521283  0.02534719]\n",
      " [ 0.00868776  0.00718991  0.41282085  0.00160545  0.45157421  0.05418848\n",
      "   0.03342432  0.00116977  0.02836126  0.00097803]\n",
      " [ 0.03431149  0.21130052  0.01234312  0.20714468  0.03150428  0.00456545\n",
      "   0.02152766  0.02726845  0.4360297   0.01400462]\n",
      " [ 0.04266089  0.00282881  0.00051199  0.0009528   0.0005501   0.00049718\n",
      "   0.00384151  0.00182076  0.01507927  0.93125665]\n",
      " [ 0.00776575  0.02826698  0.27720121  0.465437    0.09692347  0.05633432\n",
      "   0.0396429   0.01746426  0.00273342  0.00823078]\n",
      " [ 0.00484858  0.10820886  0.00027666  0.00419671  0.00267904  0.00362963\n",
      "   0.00240698  0.87298     0.00030372  0.00046984]\n",
      " [ 0.01347491  0.01372088  0.09899536  0.01648997  0.01066291  0.00139782\n",
      "   0.81807786  0.02131016  0.00072773  0.00514247]\n",
      " [ 0.00409472  0.03377562  0.11862013  0.06080336  0.01687153  0.00060875\n",
      "   0.75734395  0.00438393  0.00172119  0.0017768 ]\n",
      " [ 0.0472958   0.08162396  0.00648287  0.01688873  0.01392924  0.02134999\n",
      "   0.00511276  0.79575992  0.00349756  0.00805923]\n",
      " [ 0.16674346  0.1741679   0.03212721  0.07430423  0.11910825  0.08887004\n",
      "   0.02609346  0.06633109  0.18997735  0.06227702]\n",
      " [ 0.00017226  0.00054702  0.93792188  0.006559    0.00643077  0.00182126\n",
      "   0.04480312  0.00072375  0.00074073  0.00028031]\n",
      " [ 0.07249676  0.10143273  0.01072531  0.12050708  0.00393356  0.00123545\n",
      "   0.11417466  0.54922622  0.00775105  0.01851715]\n",
      " [ 0.06235695  0.04334383  0.17231673  0.3060573   0.02330457  0.06707857\n",
      "   0.21557252  0.02988113  0.01328149  0.06680693]\n",
      " [ 0.00313314  0.00801478  0.05525999  0.00212166  0.24342857  0.02561715\n",
      "   0.01250471  0.00911934  0.63918412  0.00161651]\n",
      " [ 0.06049666  0.02899484  0.08846633  0.02023207  0.13607718  0.02099894\n",
      "   0.05460903  0.00578328  0.41250068  0.17184097]\n",
      " [ 0.03259194  0.65816075  0.00144549  0.00221024  0.09346066  0.00205293\n",
      "   0.00343961  0.20174262  0.00264622  0.00224952]\n",
      " [ 0.00715168  0.76316804  0.00306046  0.00928118  0.07030368  0.00542575\n",
      "   0.04327472  0.06647436  0.02927624  0.00258382]\n",
      " [ 0.02293644  0.0279171   0.28504115  0.21143709  0.10783558  0.17398179\n",
      "   0.15125462  0.01718393  0.00143236  0.00097997]\n",
      " [ 0.10973765  0.17471808  0.08154707  0.215268    0.0663466   0.06332839\n",
      "   0.07657026  0.05380166  0.07663997  0.08204234]\n",
      " [ 0.06927553  0.19582111  0.01297171  0.02524504  0.0531537   0.04917764\n",
      "   0.08756966  0.46484846  0.01016158  0.03177561]\n",
      " [ 0.1335911   0.10578489  0.06146565  0.13969922  0.03395334  0.01751974\n",
      "   0.27263963  0.08072402  0.03040998  0.12421246]\n",
      " [ 0.08063877  0.00604043  0.00094674  0.00134744  0.00659003  0.00080791\n",
      "   0.00985927  0.01500561  0.01874962  0.8600142 ]\n",
      " [ 0.17844294  0.10202901  0.04204748  0.32146329  0.03237694  0.00520329\n",
      "   0.2150667   0.03532441  0.00679803  0.06124784]\n",
      " [ 0.13483734  0.28948525  0.02092019  0.12017239  0.01160837  0.17432871\n",
      "   0.12947161  0.11615609  0.00172265  0.00129743]\n",
      " [ 0.00292445  0.07996294  0.48177305  0.07912796  0.22625926  0.07153428\n",
      "   0.04668835  0.00652191  0.00449114  0.00071661]\n",
      " [ 0.04935331  0.29070845  0.02341588  0.17666069  0.02637556  0.03602228\n",
      "   0.11736634  0.27212286  0.00397589  0.00399871]\n",
      " [ 0.00501883  0.02355605  0.46776128  0.01421966  0.30114397  0.10251905\n",
      "   0.04475473  0.03549398  0.00396465  0.00156778]\n",
      " [ 0.02648944  0.42522267  0.0103333   0.04705808  0.04048795  0.00675464\n",
      "   0.0328948   0.37657329  0.0199909   0.014195  ]\n",
      " [ 0.18938226  0.02259493  0.05716074  0.0191073   0.03354141  0.15446965\n",
      "   0.03543509  0.21017608  0.11097974  0.16715288]\n",
      " [ 0.00751466  0.85107458  0.00520121  0.02957225  0.03274054  0.00616117\n",
      "   0.00793682  0.04453465  0.01158781  0.00367628]\n",
      " [ 0.00021396  0.000353    0.94643658  0.00337248  0.02568218  0.00910697\n",
      "   0.0139788   0.00030742  0.00046972  0.00007897]\n",
      " [ 0.67829728  0.07873213  0.01449323  0.01498518  0.02822733  0.00376723\n",
      "   0.0739299   0.06902085  0.02432721  0.01421975]\n",
      " [ 0.02312274  0.03382783  0.26995635  0.14781523  0.00737966  0.00172086\n",
      "   0.12245927  0.0045403   0.02457745  0.3646003 ]\n",
      " [ 0.00099985  0.01780937  0.02520061  0.00090404  0.91535211  0.01364804\n",
      "   0.00973695  0.01266075  0.00359788  0.00009043]\n",
      " [ 0.29952741  0.03086592  0.00025878  0.00934959  0.00022513  0.00012199\n",
      "   0.02548255  0.02261109  0.0039671   0.6075905 ]\n",
      " [ 0.00017072  0.00314898  0.0076577   0.0002272   0.91406971  0.06949679\n",
      "   0.00094284  0.00232239  0.00195828  0.00000543]\n",
      " [ 0.0891135   0.08398331  0.04127861  0.2582843   0.07897378  0.07006179\n",
      "   0.03939464  0.11130301  0.03435273  0.19325434]\n",
      " [ 0.57691026  0.05639438  0.01774472  0.12297079  0.01427282  0.001155\n",
      "   0.08933584  0.00697721  0.05447625  0.05976278]\n",
      " [ 0.0011804   0.05809999  0.00375018  0.9094159   0.00470973  0.00154666\n",
      "   0.01341265  0.00653221  0.00030433  0.00104791]\n",
      " [ 0.01434951  0.02597289  0.00040094  0.01452927  0.00342661  0.00341382\n",
      "   0.00458854  0.9321537   0.00025314  0.00091147]\n",
      " [ 0.02406592  0.71847898  0.00104748  0.06100399  0.02451942  0.00248757\n",
      "   0.00459869  0.15757622  0.00369356  0.00252814]\n",
      " [ 0.08041405  0.06635981  0.03178203  0.01448354  0.07750642  0.08824323\n",
      "   0.01382442  0.60304821  0.02274305  0.00159526]\n",
      " [ 0.04977756  0.07420418  0.33693057  0.16759537  0.03049177  0.00617773\n",
      "   0.2770901   0.01672237  0.03299664  0.00801377]\n",
      " [ 0.25489429  0.06387696  0.02690282  0.08529811  0.01871308  0.02465264\n",
      "   0.0553828   0.10353564  0.03020242  0.33654121]\n",
      " [ 0.01500435  0.00671386  0.00013046  0.00132687  0.00062868  0.00008919\n",
      "   0.00180654  0.00066665  0.13522144  0.83841199]\n",
      " [ 0.00210861  0.05508615  0.00438458  0.9017005   0.00438413  0.00048186\n",
      "   0.0210612   0.00970447  0.00015417  0.00093433]\n",
      " [ 0.00366465  0.0376852   0.00035657  0.00272887  0.00718667  0.00058673\n",
      "   0.0034226   0.94333929  0.000249    0.00078041]\n",
      " [ 0.00091512  0.0509038   0.01827366  0.00154868  0.85280919  0.02178833\n",
      "   0.04414228  0.00794337  0.00154973  0.00012579]\n",
      " [ 0.17528911  0.08767652  0.01547189  0.13895729  0.03114671  0.0184433\n",
      "   0.06751495  0.03108866  0.39450094  0.03991061]\n",
      " [ 0.00222081  0.00239051  0.0661141   0.02513057  0.00215352  0.00113363\n",
      "   0.89752263  0.00174805  0.00030837  0.00127789]\n",
      " [ 0.01160355  0.0469373   0.0043148   0.02461518  0.01884367  0.0057182\n",
      "   0.01011342  0.8681826   0.00188125  0.00779011]\n",
      " [ 0.00531176  0.02510239  0.09560032  0.0033449   0.12142624  0.68636656\n",
      "   0.01621049  0.04333616  0.00287676  0.00042452]\n",
      " [ 0.08932681  0.0306387   0.05370022  0.05251712  0.05338137  0.0646204\n",
      "   0.11193863  0.0498307   0.08505634  0.40898964]\n",
      " [ 0.00527348  0.00276198  0.00554868  0.00031991  0.0819601   0.89953703\n",
      "   0.00045824  0.00261016  0.00148782  0.00004264]\n",
      " [ 0.02152302  0.08129642  0.00883745  0.02390645  0.02354349  0.00223969\n",
      "   0.04840966  0.01814392  0.76965213  0.00244769]\n",
      " [ 0.07932369  0.04593978  0.16792881  0.07710039  0.0559695   0.06173839\n",
      "   0.1608682   0.04731189  0.16055284  0.1432665 ]\n",
      " [ 0.00053545  0.02110672  0.01571699  0.87714189  0.00079312  0.00051996\n",
      "   0.07721254  0.00251102  0.00105442  0.00340797]\n",
      " [ 0.07715818  0.10187677  0.00503739  0.0103914   0.22488657  0.03393716\n",
      "   0.0113677   0.10473433  0.30729514  0.12331541]\n",
      " [ 0.08883576  0.01666628  0.04399454  0.00573221  0.04629957  0.2008794\n",
      "   0.02079003  0.05484016  0.48354855  0.03841352]\n",
      " [ 0.0302829   0.02589406  0.01966874  0.01233058  0.20804167  0.66806364\n",
      "   0.00591549  0.02755688  0.00105238  0.00119368]\n",
      " [ 0.03289118  0.01535277  0.0003387   0.00039394  0.04465254  0.14813901\n",
      "   0.00104374  0.00304315  0.52171707  0.23242797]\n",
      " [ 0.00907433  0.02311219  0.44564387  0.12347263  0.09058955  0.0558516\n",
      "   0.20267712  0.00090428  0.04632073  0.00235373]\n",
      " [ 0.00064766  0.01542275  0.0101121   0.88754618  0.00142029  0.00036105\n",
      "   0.07658979  0.00263373  0.00075097  0.00451546]\n",
      " [ 0.07385182  0.0600143   0.00979532  0.07473004  0.01096672  0.00459869\n",
      "   0.37783799  0.01546967  0.30360875  0.06912658]\n",
      " [ 0.00126291  0.01155002  0.13030051  0.01680106  0.21454211  0.5857752\n",
      "   0.00845895  0.00407562  0.02674819  0.00048555]\n",
      " [ 0.00106485  0.0054668   0.00259975  0.91677183  0.00039497  0.00015974\n",
      "   0.0696248   0.00277768  0.00025828  0.00088123]\n",
      " [ 0.01243121  0.14942469  0.0120697   0.68798494  0.00556716  0.00399977\n",
      "   0.0920281   0.03100003  0.0018315   0.00366285]\n",
      " [ 0.00075287  0.00233653  0.03189726  0.0035941   0.29203776  0.6445201\n",
      "   0.00161508  0.00076798  0.02235713  0.00012117]\n",
      " [ 0.01454903  0.48515937  0.01228919  0.019485    0.05044948  0.03206206\n",
      "   0.01247041  0.15731795  0.19116354  0.02505406]\n",
      " [ 0.01589159  0.00639523  0.00503416  0.00227895  0.05100429  0.02419583\n",
      "   0.01009785  0.00328239  0.78024209  0.10157767]\n",
      " [ 0.01324917  0.00038442  0.00090277  0.00014397  0.00291671  0.04482664\n",
      "   0.00006976  0.00082296  0.92277879  0.01390478]\n",
      " [ 0.37006426  0.03123139  0.07518806  0.07172445  0.0423782   0.0534699\n",
      "   0.04775364  0.02112822  0.17085245  0.11620934]\n",
      " [ 0.01469041  0.82841933  0.00375286  0.01647256  0.04482473  0.00538004\n",
      "   0.04858402  0.03400105  0.00114634  0.00272858]\n",
      " [ 0.00044566  0.00075684  0.96925527  0.00191409  0.02065982  0.00037596\n",
      "   0.00480118  0.00002976  0.00173196  0.00002937]\n",
      " [ 0.02698124  0.00340583  0.26750648  0.05625009  0.0542932   0.52538216\n",
      "   0.03303469  0.02167785  0.00853947  0.00292905]\n",
      " [ 0.17646891  0.11905126  0.00348096  0.02078117  0.37451828  0.09364404\n",
      "   0.02450395  0.15136951  0.03459405  0.0015879 ]\n",
      " [ 0.02376755  0.16951144  0.00265158  0.01686378  0.01051625  0.00630641\n",
      "   0.01321432  0.75335938  0.00104342  0.00276597]\n",
      " [ 0.01666571  0.04672107  0.13771102  0.06734469  0.02057969  0.0059619\n",
      "   0.68539894  0.00958978  0.00277917  0.00724802]\n",
      " [ 0.00701929  0.01591205  0.00604126  0.01515236  0.07059154  0.0559806\n",
      "   0.0067428   0.00579854  0.74180251  0.07495895]\n",
      " [ 0.48607519  0.11466742  0.00121844  0.03348207  0.00340522  0.00172172\n",
      "   0.00994562  0.27809492  0.00839599  0.06299339]\n",
      " [ 0.32679686  0.1471729   0.00963851  0.02198822  0.05018053  0.00178425\n",
      "   0.07996663  0.30838743  0.04485696  0.00922778]\n",
      " [ 0.00001928  0.00023938  0.97378212  0.00083086  0.01682684  0.00055462\n",
      "   0.00762493  0.0000116   0.00010768  0.00000271]\n",
      " [ 0.00010646  0.00116776  0.70265079  0.16394497  0.01011461  0.00069267\n",
      "   0.12066435  0.00004555  0.00047076  0.00014215]\n",
      " [ 0.0032331   0.04849738  0.09512396  0.01096239  0.65723133  0.16004463\n",
      "   0.00599196  0.01689425  0.00197484  0.00004617]\n",
      " [ 0.03822499  0.04205808  0.01451661  0.01185154  0.01691041  0.00519977\n",
      "   0.03216662  0.03665985  0.80102623  0.00138592]\n",
      " [ 0.04419588  0.02635457  0.02558583  0.00638866  0.08523054  0.78686023\n",
      "   0.00388457  0.0082948   0.00986181  0.00334324]\n",
      " [ 0.00623434  0.00990605  0.00396292  0.00167045  0.02237124  0.00775789\n",
      "   0.01243224  0.00050324  0.89509594  0.0400657 ]\n",
      " [ 0.06853794  0.04502736  0.20985463  0.17696212  0.04695442  0.0789633\n",
      "   0.19719326  0.09453706  0.03414666  0.04782322]\n",
      " [ 0.0535354   0.0490029   0.13950308  0.12752473  0.04313794  0.00031395\n",
      "   0.57594234  0.00241298  0.00577268  0.00285404]\n",
      " [ 0.01824222  0.01032724  0.00564535  0.00290836  0.01107714  0.02365391\n",
      "   0.00535705  0.00922401  0.35689771  0.55666703]\n",
      " [ 0.08185145  0.33571261  0.00697933  0.01157732  0.27046102  0.03388567\n",
      "   0.04291665  0.04904155  0.15896271  0.00861171]\n",
      " [ 0.96360064  0.01204621  0.00105251  0.00189333  0.00164285  0.00047522\n",
      "   0.00418255  0.0029351   0.00136952  0.01080206]\n",
      " [ 0.19798324  0.12835327  0.02300923  0.01375522  0.03061411  0.10809387\n",
      "   0.10571188  0.1247654   0.04501001  0.2227038 ]\n",
      " [ 0.00648485  0.01606312  0.13859703  0.04903155  0.02621598  0.0026238\n",
      "   0.75568622  0.0019248   0.00085002  0.00252275]\n",
      " [ 0.73547161  0.0568926   0.00262492  0.00390261  0.07980896  0.00961316\n",
      "   0.02886324  0.00614101  0.07042011  0.00626187]\n",
      " [ 0.00061183  0.00214063  0.00599805  0.00368358  0.18823147  0.78376752\n",
      "   0.00207564  0.01294998  0.00052713  0.00001414]\n",
      " [ 0.00008567  0.00156368  0.02279221  0.00277383  0.35441938  0.61332482\n",
      "   0.00235839  0.00153196  0.00112484  0.00002522]] (21.528 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 4.52709\n",
      "INFO:tensorflow:loss = 0.975401, step = 2601 (22.089 sec)\n",
      "INFO:tensorflow:probabilities = [[ 0.00492817  0.09700635  0.22457154  0.00751085  0.58354497  0.0224287\n",
      "   0.01305441  0.0069514   0.03612808  0.00387554]\n",
      " [ 0.05114162  0.26824391  0.00124947  0.00889038  0.01611776  0.00776323\n",
      "   0.01315596  0.62830108  0.00079018  0.00434636]\n",
      " [ 0.00032146  0.02872117  0.00737534  0.89755201  0.00266622  0.00049029\n",
      "   0.06096895  0.00041276  0.00028856  0.0012033 ]\n",
      " [ 0.01016904  0.01584224  0.00274201  0.006769    0.08328256  0.4303169\n",
      "   0.00566771  0.03007705  0.40706778  0.00806571]\n",
      " [ 0.99440825  0.00104705  0.00005662  0.00050507  0.0005105   0.0006221\n",
      "   0.00075696  0.00110259  0.00079804  0.00019293]\n",
      " [ 0.00008092  0.00124538  0.00423424  0.00225946  0.06389496  0.91790128\n",
      "   0.00465521  0.00538535  0.00033263  0.00001063]\n",
      " [ 0.33814383  0.07189411  0.00057641  0.05831837  0.00134861  0.00301701\n",
      "   0.01984495  0.01734683  0.0148927   0.47461706]\n",
      " [ 0.13961408  0.02367467  0.00231544  0.02963614  0.01568743  0.02491872\n",
      "   0.00454516  0.07905496  0.52959818  0.15095535]\n",
      " [ 0.00309512  0.05099066  0.0253385   0.83018243  0.01187137  0.00683724\n",
      "   0.05023418  0.01163764  0.00170477  0.00810815]\n",
      " [ 0.0222695   0.09317171  0.00029599  0.01537923  0.00181108  0.00583916\n",
      "   0.0063352   0.84869272  0.00090929  0.00529599]\n",
      " [ 0.020786    0.05006343  0.00624634  0.01994898  0.03148021  0.0025884\n",
      "   0.03616783  0.02588264  0.79723006  0.00960609]\n",
      " [ 0.00744953  0.08509792  0.00814792  0.86239874  0.00858298  0.00579354\n",
      "   0.01165008  0.00488588  0.00049016  0.00550321]\n",
      " [ 0.01203107  0.0058364   0.00383072  0.00059996  0.0082489   0.01108272\n",
      "   0.01172427  0.00370955  0.74233222  0.20060416]\n",
      " [ 0.36622003  0.26168001  0.02212308  0.05616036  0.00686451  0.00956617\n",
      "   0.12584163  0.01301705  0.09852127  0.04000596]\n",
      " [ 0.12169472  0.08896127  0.0078886   0.08916831  0.04686802  0.0470827\n",
      "   0.06623244  0.4786095   0.00733353  0.046161  ]\n",
      " [ 0.07876448  0.08955529  0.09765488  0.05828565  0.03571848  0.07128194\n",
      "   0.18036754  0.0156375   0.21593848  0.15679573]\n",
      " [ 0.05528428  0.11282566  0.0072045   0.0581931   0.0160456   0.00555565\n",
      "   0.02736715  0.01178328  0.68789035  0.01785036]\n",
      " [ 0.002712    0.01736411  0.15457472  0.22782049  0.01192136  0.45685625\n",
      "   0.00383934  0.08593296  0.01407637  0.02490243]\n",
      " [ 0.90205151  0.02571398  0.0015599   0.00066948  0.01580934  0.00550573\n",
      "   0.00834484  0.03080106  0.00539896  0.0041451 ]\n",
      " [ 0.13416539  0.07439577  0.04082889  0.03030068  0.05036629  0.06078737\n",
      "   0.05866613  0.09044565  0.17833126  0.28171253]\n",
      " [ 0.00413758  0.03515154  0.02636282  0.805978    0.00428967  0.00042549\n",
      "   0.08292099  0.00147984  0.00142237  0.03783176]\n",
      " [ 0.1346897   0.03108572  0.02174984  0.18550207  0.00823933  0.03041872\n",
      "   0.48035938  0.09202299  0.00530451  0.01062778]\n",
      " [ 0.05460453  0.13084152  0.01366181  0.02050264  0.0182923   0.00565029\n",
      "   0.01863369  0.10258476  0.6212852   0.0139432 ]\n",
      " [ 0.00532262  0.00187802  0.79253477  0.00468559  0.04709257  0.04882502\n",
      "   0.08689565  0.0068085   0.00389372  0.00206358]\n",
      " [ 0.16923408  0.07294144  0.01053269  0.15944374  0.04017137  0.21764715\n",
      "   0.03648308  0.15021764  0.13331631  0.01001244]\n",
      " [ 0.7171011   0.03112499  0.01661372  0.00856161  0.06631351  0.01495929\n",
      "   0.02678752  0.07202061  0.0213198   0.02519784]\n",
      " [ 0.05454581  0.07766899  0.02835155  0.12975469  0.01717164  0.00668394\n",
      "   0.66003549  0.00260085  0.01394093  0.0092461 ]\n",
      " [ 0.00328139  0.01088381  0.01929105  0.01266204  0.72241426  0.10904913\n",
      "   0.00840574  0.11115277  0.00261385  0.00024601]\n",
      " [ 0.01159617  0.01950731  0.03625889  0.01424424  0.00721121  0.00288386\n",
      "   0.88899606  0.01238394  0.00157711  0.00534111]\n",
      " [ 0.00237625  0.05281676  0.11607333  0.04626057  0.00689107  0.00076783\n",
      "   0.77066588  0.00271543  0.00033236  0.00110044]\n",
      " [ 0.01291423  0.62493807  0.00285312  0.0398804   0.0465203   0.00458978\n",
      "   0.14607021  0.09593665  0.01807198  0.00822531]\n",
      " [ 0.00055025  0.00151645  0.00072225  0.00013866  0.00394743  0.00280376\n",
      "   0.0005237   0.00009146  0.98356503  0.00614101]\n",
      " [ 0.00023619  0.00122121  0.81584245  0.01407198  0.08666019  0.07223186\n",
      "   0.00740368  0.00079339  0.00150858  0.00003039]\n",
      " [ 0.0362052   0.1777378   0.0297555   0.05307427  0.07247209  0.02840174\n",
      "   0.18932563  0.02811212  0.3789117   0.00600401]\n",
      " [ 0.05087252  0.01777789  0.03663289  0.27606526  0.04185902  0.25776964\n",
      "   0.21928377  0.03763043  0.02043801  0.04167055]\n",
      " [ 0.98262388  0.00123755  0.00012116  0.00028445  0.00866891  0.00132029\n",
      "   0.0004048   0.00276094  0.00207011  0.00050793]\n",
      " [ 0.00062492  0.00229817  0.08577055  0.01109331  0.00806187  0.00021848\n",
      "   0.89127547  0.00045802  0.00009228  0.00010696]\n",
      " [ 0.00249765  0.09710559  0.00324731  0.81498295  0.00109221  0.00040865\n",
      "   0.07337089  0.00314795  0.00053539  0.00361142]\n",
      " [ 0.1687343   0.03281667  0.03753078  0.1464338   0.0349118   0.01178161\n",
      "   0.41652468  0.03438366  0.0172735   0.09960922]\n",
      " [ 0.01784075  0.03529956  0.01016641  0.02906404  0.02159703  0.00730459\n",
      "   0.00961852  0.03219604  0.82047278  0.01644021]\n",
      " [ 0.00050841  0.00287438  0.00273315  0.0022956   0.0359711   0.94860274\n",
      "   0.00130001  0.00430891  0.00123237  0.00017339]\n",
      " [ 0.00225067  0.07553248  0.04324906  0.00304569  0.76568156  0.02384801\n",
      "   0.0029318   0.00474801  0.07612033  0.0025923 ]\n",
      " [ 0.00107844  0.00410984  0.0438264   0.00721905  0.00308851  0.00010352\n",
      "   0.93641776  0.00358515  0.00010262  0.00046877]\n",
      " [ 0.00519626  0.02564954  0.22231127  0.5760892   0.0071645   0.03102268\n",
      "   0.10698584  0.00763958  0.00633215  0.01160901]\n",
      " [ 0.01024748  0.00440442  0.02167721  0.47687441  0.00391945  0.01807277\n",
      "   0.37731472  0.07738195  0.00611012  0.00399757]\n",
      " [ 0.68858528  0.01556261  0.00000568  0.00163241  0.00016236  0.0000435\n",
      "   0.00040968  0.01301022  0.01738307  0.2632052 ]\n",
      " [ 0.12032967  0.06056948  0.01806924  0.03222232  0.03824433  0.38852802\n",
      "   0.08396558  0.20526601  0.04346899  0.00933642]\n",
      " [ 0.01324973  0.07594578  0.00759651  0.01341117  0.0148807   0.01345059\n",
      "   0.03625997  0.00540488  0.81723177  0.0025689 ]\n",
      " [ 0.0645726   0.50048721  0.00177041  0.033383    0.00831816  0.00130358\n",
      "   0.01036534  0.0808188   0.0337458   0.26523513]\n",
      " [ 0.00383951  0.00275533  0.00866412  0.00119001  0.0245946   0.0282187\n",
      "   0.00135725  0.00022575  0.9181866   0.01096805]\n",
      " [ 0.03608212  0.77312869  0.00099709  0.05987309  0.02433169  0.00348816\n",
      "   0.00765554  0.08944665  0.00120286  0.00379417]\n",
      " [ 0.08894151  0.30935079  0.00198277  0.005336    0.04024298  0.00281126\n",
      "   0.01864043  0.50470638  0.02618859  0.00179929]\n",
      " [ 0.00125378  0.00907829  0.03517548  0.01841306  0.02189539  0.00134139\n",
      "   0.90564555  0.00307441  0.00254974  0.00157284]\n",
      " [ 0.01384628  0.58479369  0.00365933  0.23840459  0.01178082  0.00359125\n",
      "   0.04434066  0.09636613  0.00054912  0.00266813]\n",
      " [ 0.01136443  0.13523866  0.01389888  0.70995128  0.01812267  0.00896516\n",
      "   0.04319207  0.04744245  0.00370757  0.00811693]\n",
      " [ 0.00017668  0.00161786  0.49240842  0.00064566  0.46027434  0.02331163\n",
      "   0.01640401  0.00023046  0.00491357  0.0000173 ]\n",
      " [ 0.01045713  0.05953304  0.34880874  0.04863675  0.41859463  0.01735217\n",
      "   0.05664465  0.01310347  0.0261223   0.00074715]\n",
      " [ 0.00243073  0.0214718   0.07742909  0.01228527  0.66579062  0.20908596\n",
      "   0.00126924  0.00163208  0.00834894  0.00025625]\n",
      " [ 0.26326346  0.09860797  0.02885883  0.01082374  0.06925578  0.2090459\n",
      "   0.02334925  0.05036813  0.17819141  0.06823554]\n",
      " [ 0.94567752  0.00088538  0.00004023  0.0022633   0.00069501  0.00017908\n",
      "   0.00007836  0.00821842  0.02098268  0.02097997]\n",
      " [ 0.01127094  0.01427368  0.00123929  0.00141599  0.00437069  0.00929138\n",
      "   0.00313054  0.00155168  0.17345878  0.77999699]\n",
      " [ 0.81259805  0.07708825  0.0002327   0.00579467  0.00055292  0.00115892\n",
      "   0.0019365   0.06662081  0.00065537  0.03336186]\n",
      " [ 0.09052025  0.39324704  0.00358934  0.00758196  0.08864452  0.00247678\n",
      "   0.35014763  0.05496304  0.00502837  0.00380102]\n",
      " [ 0.0303283   0.02295118  0.00214506  0.15340266  0.0009822   0.00015588\n",
      "   0.00566226  0.01749764  0.00565138  0.76122332]\n",
      " [ 0.00286837  0.00154777  0.0015585   0.0001628   0.01372684  0.00178098\n",
      "   0.00091827  0.00027497  0.97081113  0.00635027]\n",
      " [ 0.00131614  0.004456    0.58699751  0.00566242  0.26542184  0.08499407\n",
      "   0.04387829  0.0017212   0.00513736  0.00041506]\n",
      " [ 0.07204696  0.10095253  0.02179977  0.5998947   0.01186759  0.02854515\n",
      "   0.03115448  0.00539959  0.02476829  0.10357098]\n",
      " [ 0.00617808  0.15355948  0.11289885  0.00571827  0.63965517  0.01816101\n",
      "   0.02648166  0.00124469  0.0353403   0.00076251]\n",
      " [ 0.04573638  0.04379417  0.07001756  0.26916292  0.03465262  0.08126619\n",
      "   0.15291375  0.05967171  0.12570727  0.11707741]\n",
      " [ 0.00169058  0.03028577  0.00414913  0.93654704  0.00308592  0.00308001\n",
      "   0.01288763  0.00769292  0.00027666  0.00030429]\n",
      " [ 0.0286437   0.13915303  0.0408301   0.460186    0.02082969  0.00261422\n",
      "   0.16074054  0.02133529  0.08187668  0.04379088]\n",
      " [ 0.03870766  0.10235106  0.05171762  0.035158    0.04199001  0.00338081\n",
      "   0.64280874  0.02281861  0.05924539  0.00182209]\n",
      " [ 0.10686564  0.08333271  0.02494706  0.02102472  0.21021691  0.16116185\n",
      "   0.04514816  0.09826992  0.17048575  0.07854736]\n",
      " [ 0.979231    0.00615118  0.00030533  0.00062681  0.00194716  0.0001695\n",
      "   0.00093713  0.00818466  0.00083426  0.00161297]\n",
      " [ 0.04517844  0.01130545  0.00474467  0.0213044   0.01286164  0.00785471\n",
      "   0.01420998  0.0237994   0.04022799  0.81851339]\n",
      " [ 0.06830175  0.3996391   0.00519999  0.03630742  0.03618092  0.01424127\n",
      "   0.01158858  0.41000748  0.01325159  0.00528184]\n",
      " [ 0.31906462  0.22898048  0.00292559  0.07686182  0.01245517  0.00184303\n",
      "   0.08997982  0.19855432  0.01457427  0.05476083]\n",
      " [ 0.24988721  0.13048896  0.01978376  0.13251452  0.03132074  0.00191148\n",
      "   0.20890805  0.18051831  0.00339674  0.04127023]\n",
      " [ 0.00554682  0.04993905  0.05408433  0.00885431  0.67777473  0.05758091\n",
      "   0.11122616  0.02417961  0.00745836  0.00335566]\n",
      " [ 0.02439611  0.04336865  0.00013842  0.00617328  0.00076008  0.00008813\n",
      "   0.01046159  0.0030427   0.00592715  0.90564388]\n",
      " [ 0.00508258  0.00572038  0.00927106  0.00342483  0.07345491  0.77744108\n",
      "   0.00856541  0.00602723  0.11063243  0.00038011]\n",
      " [ 0.02285949  0.00423472  0.00049095  0.00057289  0.00846014  0.01834002\n",
      "   0.0009163   0.00749455  0.78048742  0.15614349]\n",
      " [ 0.03384468  0.12179121  0.0040703   0.15587258  0.00319158  0.00987531\n",
      "   0.04073341  0.60782129  0.00083726  0.02196241]\n",
      " [ 0.00737401  0.01890943  0.39323866  0.09843407  0.14156237  0.2009566\n",
      "   0.07068576  0.06149822  0.00488486  0.00245604]\n",
      " [ 0.03029509  0.59518999  0.00280745  0.00481266  0.12066904  0.0154049\n",
      "   0.00406162  0.03654677  0.17871101  0.01150147]\n",
      " [ 0.00025908  0.00103987  0.00399145  0.00123967  0.03467132  0.95607644\n",
      "   0.00085072  0.00153253  0.00033123  0.00000777]\n",
      " [ 0.00087961  0.01794961  0.09302493  0.05557448  0.01718891  0.00015176\n",
      "   0.81355113  0.00033064  0.00021571  0.00113326]\n",
      " [ 0.01018544  0.01030888  0.01737472  0.04036758  0.02781958  0.00046891\n",
      "   0.89181441  0.0007064   0.00046479  0.00048913]\n",
      " [ 0.00327308  0.00436501  0.00005755  0.00050901  0.00009726  0.00007047\n",
      "   0.00033378  0.00108026  0.02687012  0.96334344]\n",
      " [ 0.01266442  0.00954812  0.0000545   0.00198579  0.00121074  0.00056248\n",
      "   0.00189449  0.97099662  0.00052391  0.00055878]\n",
      " [ 0.01714243  0.558716    0.00959095  0.17907768  0.01600861  0.00525959\n",
      "   0.031329    0.04352329  0.11124746  0.02810506]\n",
      " [ 0.00298596  0.00868303  0.08039726  0.0057584   0.08863879  0.77515638\n",
      "   0.00939876  0.0043297   0.0230004   0.00165132]\n",
      " [ 0.00694135  0.00267473  0.00206483  0.00298536  0.01681023  0.0221733\n",
      "   0.00115029  0.00322092  0.18672135  0.75525755]\n",
      " [ 0.00405753  0.00889192  0.00006892  0.02351431  0.0007802   0.00002253\n",
      "   0.0037444   0.00631819  0.00101288  0.95158911]\n",
      " [ 0.03730867  0.17866656  0.0064239   0.43161568  0.01075863  0.00836756\n",
      "   0.01471737  0.30399591  0.0047975   0.00334812]\n",
      " [ 0.00581215  0.05978     0.033418    0.75114012  0.01310336  0.00073222\n",
      "   0.12421689  0.00414202  0.00649553  0.00115986]\n",
      " [ 0.00028561  0.01608677  0.00493293  0.00531246  0.2434539   0.71098989\n",
      "   0.00224785  0.00280478  0.0136253   0.0002605 ]\n",
      " [ 0.00248742  0.09076402  0.05887131  0.02202811  0.51633298  0.0763183\n",
      "   0.02094138  0.02663649  0.18348832  0.00213162]\n",
      " [ 0.00465     0.00365112  0.00087197  0.00047685  0.01245795  0.01572424\n",
      "   0.00124089  0.00101103  0.83121955  0.12869638]\n",
      " [ 0.18922153  0.14461571  0.00360065  0.01112372  0.014213    0.00316964\n",
      "   0.07726338  0.53018355  0.02496718  0.00164166]] (22.089 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 5.06434\n",
      "INFO:tensorflow:loss = 0.719517, step = 2701 (19.746 sec)\n",
      "INFO:tensorflow:probabilities = [[ 0.00798648  0.0237061   0.62689769  0.02854398  0.13719159  0.03793151\n",
      "   0.0800707   0.04183488  0.00845483  0.00738219]\n",
      " [ 0.0261048   0.13975111  0.00306859  0.03546005  0.08367515  0.02371753\n",
      "   0.01346559  0.61112016  0.02224379  0.04139329]\n",
      " [ 0.10706554  0.08905989  0.02161488  0.10185704  0.04463784  0.0641025\n",
      "   0.09415996  0.09303712  0.16724342  0.21722183]\n",
      " [ 0.96145815  0.01126953  0.00041893  0.00076517  0.00636393  0.00368163\n",
      "   0.00308725  0.00693106  0.00284683  0.0031776 ]\n",
      " [ 0.00024974  0.0077826   0.00208432  0.00489234  0.26986849  0.69423932\n",
      "   0.00093048  0.0196824   0.00024549  0.00002492]\n",
      " [ 0.01147739  0.02820891  0.00096121  0.00698209  0.0168285   0.00259626\n",
      "   0.00247577  0.92767358  0.00072481  0.00207155]\n",
      " [ 0.01145676  0.54694754  0.00263444  0.032375    0.05534988  0.00526744\n",
      "   0.02645687  0.29221734  0.02585008  0.00144464]\n",
      " [ 0.00070203  0.01534836  0.54132134  0.11911961  0.02658398  0.00222948\n",
      "   0.28401491  0.00097934  0.00954517  0.00015579]\n",
      " [ 0.00562751  0.01051292  0.01797963  0.00474066  0.03892305  0.00093866\n",
      "   0.90239686  0.00781927  0.01030916  0.00075237]\n",
      " [ 0.00119323  0.01543468  0.11083488  0.84444815  0.00549806  0.00067118\n",
      "   0.01549008  0.0013021   0.00271722  0.00241045]\n",
      " [ 0.03472387  0.00233012  0.18849058  0.0341193   0.06947471  0.64683175\n",
      "   0.01083076  0.00601     0.00623214  0.00095681]\n",
      " [ 0.00107348  0.00672884  0.00271579  0.94904804  0.00035734  0.00017906\n",
      "   0.03200727  0.00554869  0.00060106  0.00174039]\n",
      " [ 0.08402272  0.08096145  0.2013678   0.11910428  0.06755695  0.0985953\n",
      "   0.1357594   0.03114322  0.04110763  0.14038125]\n",
      " [ 0.00104785  0.01851828  0.01113807  0.92672038  0.00684867  0.00106291\n",
      "   0.02471306  0.00789919  0.00027164  0.0017798 ]\n",
      " [ 0.00975254  0.41951832  0.0317104   0.09662446  0.1075435   0.01061027\n",
      "   0.08347338  0.21899502  0.00507772  0.01669433]\n",
      " [ 0.03304807  0.00269536  0.00001168  0.00111345  0.00001792  0.00000793\n",
      "   0.00015015  0.00049876  0.00416033  0.95829636]\n",
      " [ 0.07117543  0.11158799  0.01498481  0.19915809  0.05610863  0.0063196\n",
      "   0.24034289  0.04293416  0.25010574  0.00728265]\n",
      " [ 0.09879895  0.11585235  0.00733918  0.0159608   0.05276206  0.18937834\n",
      "   0.00805707  0.3936047   0.09087517  0.02737138]\n",
      " [ 0.01167375  0.06261621  0.5871473   0.04081257  0.06981103  0.02623642\n",
      "   0.13523676  0.00423454  0.03881792  0.02341344]\n",
      " [ 0.00905859  0.03560562  0.03130635  0.00184782  0.53517818  0.00937971\n",
      "   0.01913309  0.0046682   0.35068685  0.00313563]\n",
      " [ 0.72209966  0.02585041  0.01326148  0.01777737  0.01105507  0.00650665\n",
      "   0.1843244   0.01219321  0.00467595  0.00225576]\n",
      " [ 0.06166462  0.33496478  0.0103236   0.02147023  0.38606274  0.02177951\n",
      "   0.00302815  0.12671158  0.02916862  0.00482619]\n",
      " [ 0.0807892   0.06572989  0.00613995  0.00219052  0.71325308  0.09067712\n",
      "   0.00308514  0.02959922  0.00813487  0.00040105]\n",
      " [ 0.00110578  0.01574914  0.00323264  0.00516     0.06523304  0.01105831\n",
      "   0.0026808   0.00217914  0.86891931  0.02468177]\n",
      " [ 0.00219198  0.02255386  0.00538076  0.93700695  0.00193781  0.00039831\n",
      "   0.02419404  0.0054055   0.00004025  0.00089058]\n",
      " [ 0.02445027  0.02408915  0.03673326  0.54073149  0.00178766  0.00642392\n",
      "   0.35358247  0.00198722  0.00407664  0.00613786]\n",
      " [ 0.03467338  0.06907739  0.01860352  0.03205924  0.21437247  0.58507651\n",
      "   0.00481394  0.02756761  0.01121302  0.00254287]\n",
      " [ 0.02139458  0.19095513  0.00041646  0.0109487   0.01273276  0.00325075\n",
      "   0.01578712  0.74084747  0.0018368   0.0018302 ]\n",
      " [ 0.00059994  0.93596941  0.00077688  0.01951211  0.00659277  0.00066959\n",
      "   0.00836711  0.02676149  0.00018317  0.00056764]\n",
      " [ 0.01510029  0.05224542  0.00935248  0.22618425  0.0088022   0.00034705\n",
      "   0.02847105  0.06748846  0.0017475   0.59026128]\n",
      " [ 0.99707913  0.00079108  0.00002275  0.00031061  0.00003617  0.00004483\n",
      "   0.00021525  0.0008433   0.00047325  0.00018375]\n",
      " [ 0.0000732   0.00319364  0.79346341  0.05969089  0.01375422  0.00034393\n",
      "   0.12843361  0.0000576   0.00067475  0.00031478]\n",
      " [ 0.00013755  0.00300391  0.0133625   0.95748025  0.00008356  0.00004434\n",
      "   0.02493248  0.0004451   0.00005868  0.00045164]\n",
      " [ 0.01819435  0.1586234   0.14529821  0.17395423  0.13373747  0.05316417\n",
      "   0.20583653  0.03523512  0.03625907  0.03969746]\n",
      " [ 0.00150302  0.00118114  0.00089857  0.00013535  0.04629871  0.94747639\n",
      "   0.00088389  0.00048767  0.00111152  0.00002384]\n",
      " [ 0.00690902  0.90305513  0.00057244  0.02090695  0.02379913  0.00178981\n",
      "   0.00936945  0.02931606  0.00376373  0.00051828]\n",
      " [ 0.00440762  0.00657501  0.01440986  0.0161713   0.00230059  0.00007045\n",
      "   0.95350879  0.00088453  0.00028243  0.0013895 ]\n",
      " [ 0.00807545  0.02002554  0.3369      0.06386033  0.00437735  0.00091445\n",
      "   0.55787748  0.00247642  0.00403403  0.00145899]\n",
      " [ 0.00126881  0.02049994  0.12043286  0.09555332  0.02069147  0.00025397\n",
      "   0.73754632  0.00156321  0.00178595  0.00040408]\n",
      " [ 0.00022303  0.00254196  0.86485177  0.01345089  0.0714441   0.00423284\n",
      "   0.04221113  0.00037853  0.00064547  0.00002026]\n",
      " [ 0.00125738  0.06854635  0.01345432  0.00786635  0.63041282  0.20318849\n",
      "   0.0147677   0.00290498  0.05015878  0.00744295]\n",
      " [ 0.01568352  0.08636101  0.16363592  0.01992944  0.56510311  0.01170305\n",
      "   0.06662893  0.01741899  0.04753998  0.00599603]\n",
      " [ 0.07312512  0.05458977  0.15833974  0.52420115  0.0282112   0.01168012\n",
      "   0.08072168  0.01434673  0.00908097  0.04570347]\n",
      " [ 0.00804584  0.02230178  0.0363109   0.86543894  0.01003051  0.00489318\n",
      "   0.03649549  0.00765185  0.00235762  0.00647376]\n",
      " [ 0.43372735  0.0423211   0.06001368  0.06273937  0.05759616  0.06847519\n",
      "   0.04546285  0.09895868  0.04275935  0.0879463 ]\n",
      " [ 0.00912903  0.85676235  0.0012131   0.0080875   0.01055882  0.00155937\n",
      "   0.0205272   0.08509553  0.00663092  0.00043628]\n",
      " [ 0.94697195  0.02588255  0.00001178  0.00092915  0.00021711  0.00013494\n",
      "   0.00199154  0.00552642  0.0030558   0.01527876]\n",
      " [ 0.05656964  0.01372198  0.37468296  0.06558201  0.19732001  0.08820484\n",
      "   0.10702141  0.01518583  0.03256346  0.04914789]\n",
      " [ 0.00069272  0.00801108  0.0068765   0.96754688  0.00063627  0.00037678\n",
      "   0.01135933  0.00288598  0.00053424  0.00108012]\n",
      " [ 0.03624581  0.37439844  0.0248427   0.03218042  0.08671965  0.02893222\n",
      "   0.0099671   0.02766108  0.33805168  0.04100083]\n",
      " [ 0.95595896  0.01281818  0.00016267  0.00074807  0.00626484  0.0006102\n",
      "   0.00627203  0.00343606  0.01273817  0.00099086]\n",
      " [ 0.0040138   0.02679015  0.01754721  0.00894734  0.15756093  0.60517693\n",
      "   0.00099657  0.01398013  0.16151465  0.00347235]\n",
      " [ 0.00007753  0.00060721  0.90039688  0.02099518  0.03952539  0.00426829\n",
      "   0.03262971  0.00048609  0.00053035  0.00048328]\n",
      " [ 0.0082993   0.07010517  0.00022232  0.00439059  0.00458734  0.00128301\n",
      "   0.00221125  0.9075551   0.00104198  0.00030392]\n",
      " [ 0.72673523  0.01521339  0.00021391  0.00109544  0.00456554  0.00210844\n",
      "   0.02562469  0.07132475  0.01925888  0.13385975]\n",
      " [ 0.0845597   0.04984028  0.00097581  0.00879665  0.00290152  0.00044197\n",
      "   0.00969496  0.02036651  0.00678923  0.81563336]\n",
      " [ 0.01309622  0.00093244  0.00006169  0.00200254  0.00021258  0.00000546\n",
      "   0.00040523  0.00024193  0.02112203  0.9619199 ]\n",
      " [ 0.04193627  0.16258842  0.00933207  0.01546862  0.07456701  0.46610847\n",
      "   0.07446811  0.10344861  0.03295609  0.01912642]\n",
      " [ 0.00063939  0.00762189  0.01375694  0.91344756  0.00127656  0.00007056\n",
      "   0.0592248   0.00040907  0.00115949  0.00239377]\n",
      " [ 0.03177679  0.79129571  0.00269841  0.02948446  0.05262898  0.0071765\n",
      "   0.00925542  0.06661548  0.00392244  0.00514588]\n",
      " [ 0.00191747  0.00751171  0.02875928  0.02130875  0.00163209  0.00207059\n",
      "   0.93124527  0.00069197  0.00068849  0.00417446]\n",
      " [ 0.01492069  0.23308714  0.00719896  0.61112744  0.0383583   0.00422383\n",
      "   0.05222292  0.03185874  0.00226312  0.00473893]\n",
      " [ 0.54229468  0.10763127  0.01862153  0.01453406  0.04340556  0.04656418\n",
      "   0.00448218  0.04850582  0.11083794  0.06312279]\n",
      " [ 0.00131868  0.00610143  0.0508864   0.02214555  0.00113137  0.0012528\n",
      "   0.91575831  0.0001821   0.00008312  0.00114016]\n",
      " [ 0.01188297  0.01135956  0.00322284  0.00223045  0.06052517  0.02025105\n",
      "   0.00289742  0.00287143  0.72184795  0.16291115]\n",
      " [ 0.01181576  0.0187983   0.00716125  0.00607115  0.02130397  0.01848222\n",
      "   0.00072619  0.02297575  0.31312096  0.57954443]\n",
      " [ 0.10594349  0.50221258  0.00244339  0.21526338  0.01578584  0.00646505\n",
      "   0.01321522  0.07625659  0.01970336  0.04271115]\n",
      " [ 0.00021406  0.00129918  0.00080889  0.00013198  0.00777677  0.00476033\n",
      "   0.00008722  0.00020056  0.95717436  0.02754664]\n",
      " [ 0.00634391  0.0051733   0.02923515  0.01482285  0.00294589  0.00058472\n",
      "   0.93605036  0.00351776  0.00054976  0.00077636]\n",
      " [ 0.01624041  0.01379238  0.21946131  0.27128971  0.00751484  0.00056724\n",
      "   0.33148107  0.00391322  0.13299255  0.00274723]\n",
      " [ 0.00833417  0.01164658  0.20537493  0.02081176  0.18253286  0.49873215\n",
      "   0.04106249  0.02018588  0.0107635   0.00055571]\n",
      " [ 0.00339749  0.00218403  0.00096798  0.00046445  0.01442113  0.00463628\n",
      "   0.00162198  0.00006707  0.95369363  0.01854604]\n",
      " [ 0.00127772  0.0586      0.000044    0.02950447  0.00022668  0.00002093\n",
      "   0.00153751  0.00698877  0.00062066  0.90117931]\n",
      " [ 0.00519868  0.04487845  0.05758011  0.00217309  0.60547179  0.03555333\n",
      "   0.01185412  0.00521256  0.23013723  0.00194063]\n",
      " [ 0.00252772  0.00291778  0.01156104  0.00186234  0.03502258  0.93952215\n",
      "   0.00109633  0.00527792  0.00019177  0.00002037]\n",
      " [ 0.34065571  0.05696121  0.02550741  0.16405268  0.01692105  0.01548821\n",
      "   0.30282164  0.03981492  0.01905483  0.01872232]\n",
      " [ 0.03445989  0.04222988  0.00599957  0.03383877  0.01540633  0.00479626\n",
      "   0.03272523  0.01952274  0.03000368  0.78101772]\n",
      " [ 0.01573644  0.05985634  0.00224089  0.04771532  0.00384416  0.00295546\n",
      "   0.01253109  0.85201812  0.00087041  0.00223181]\n",
      " [ 0.00087543  0.02618074  0.75564897  0.03676523  0.05247873  0.00573767\n",
      "   0.10449369  0.01682923  0.00068422  0.00030608]\n",
      " [ 0.00394451  0.00213245  0.08954331  0.00384999  0.03191635  0.00459229\n",
      "   0.84516609  0.00074718  0.01395298  0.00415488]\n",
      " [ 0.01603901  0.07920002  0.00582608  0.04120906  0.02785308  0.00361682\n",
      "   0.02629326  0.03173022  0.75188017  0.01635232]\n",
      " [ 0.01426851  0.03454224  0.00015986  0.00397567  0.00435896  0.00141515\n",
      "   0.00563329  0.93356115  0.00051805  0.00156707]\n",
      " [ 0.18209621  0.58398587  0.0007502   0.04775822  0.00752569  0.00061066\n",
      "   0.03796966  0.06982715  0.01191246  0.05756381]\n",
      " [ 0.00578103  0.0023611   0.00631027  0.00146029  0.03136525  0.0082609\n",
      "   0.0046603   0.00073844  0.92734921  0.01171314]\n",
      " [ 0.14654437  0.0437018   0.0449868   0.03989744  0.12221365  0.01060703\n",
      "   0.0833621   0.15045756  0.2741845   0.08404476]\n",
      " [ 0.03241711  0.0329909   0.00046973  0.02383681  0.00264808  0.0117879\n",
      "   0.0029219   0.88984388  0.0004692   0.00261445]\n",
      " [ 0.00011662  0.00535957  0.00191509  0.00331195  0.06927643  0.91553605\n",
      "   0.00058272  0.00165031  0.00207139  0.00017989]\n",
      " [ 0.0266042   0.00904603  0.00107685  0.00441269  0.0006468   0.00023308\n",
      "   0.02671165  0.01812504  0.00501863  0.90812492]\n",
      " [ 0.17052189  0.01253421  0.09451306  0.03249801  0.19130054  0.18488385\n",
      "   0.29164341  0.01550568  0.00528546  0.00131393]\n",
      " [ 0.71205616  0.00879699  0.00137736  0.01604942  0.00682301  0.02463659\n",
      "   0.1918195   0.02737448  0.00170463  0.00936184]\n",
      " [ 0.00265378  0.00188971  0.05460455  0.59701639  0.00026871  0.00010893\n",
      "   0.33769795  0.00009514  0.0038648   0.00180006]\n",
      " [ 0.01025996  0.04776944  0.00148948  0.02932112  0.00248483  0.01452503\n",
      "   0.01061489  0.87996519  0.00101037  0.00255973]\n",
      " [ 0.00003115  0.00040181  0.00040836  0.00004367  0.03211005  0.96439624\n",
      "   0.00080101  0.00034699  0.00145775  0.00000296]\n",
      " [ 0.57145244  0.00842374  0.00867563  0.03374372  0.01108876  0.00855614\n",
      "   0.33812192  0.01612506  0.00280844  0.00100416]\n",
      " [ 0.00843463  0.00063759  0.00045073  0.00039341  0.00029071  0.00041195\n",
      "   0.00583445  0.0002471   0.12162582  0.86167359]\n",
      " [ 0.02012084  0.00337146  0.00295195  0.00074729  0.01050796  0.01725565\n",
      "   0.00041173  0.00171999  0.92516112  0.01775205]\n",
      " [ 0.99685693  0.00093557  0.00011974  0.00006292  0.00074983  0.00020001\n",
      "   0.00012812  0.00076571  0.00016313  0.00001792]\n",
      " [ 0.0091799   0.00814207  0.00763422  0.00171555  0.02094838  0.00371519\n",
      "   0.00112205  0.00119303  0.93322456  0.01312505]\n",
      " [ 0.00362472  0.01284445  0.56860226  0.00449063  0.31601155  0.0043735\n",
      "   0.07024354  0.00128925  0.0158485   0.00267157]\n",
      " [ 0.02069752  0.01630951  0.49336731  0.33773509  0.0080477   0.00309023\n",
      "   0.09878576  0.00038854  0.01913774  0.00244068]] (19.746 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 4.84572\n",
      "INFO:tensorflow:loss = 0.973922, step = 2801 (20.636 sec)\n",
      "INFO:tensorflow:probabilities = [[ 0.03520432  0.00719671  0.00027152  0.00483918  0.00016215  0.00002889\n",
      "   0.00374384  0.00422094  0.00892939  0.93540311]\n",
      " [ 0.16585173  0.24765064  0.00206221  0.01807358  0.05326014  0.05308567\n",
      "   0.01613572  0.06162081  0.05955956  0.32269987]\n",
      " [ 0.00911785  0.20207076  0.007309    0.5535208   0.01678972  0.00041226\n",
      "   0.0879144   0.07000591  0.00538639  0.04747296]\n",
      " [ 0.00038661  0.00037102  0.07685549  0.00066833  0.09540209  0.82236826\n",
      "   0.00100603  0.00011757  0.00280894  0.00001555]\n",
      " [ 0.15090063  0.02247276  0.05286982  0.31264809  0.00134682  0.00672683\n",
      "   0.29636994  0.04803711  0.01573455  0.09289351]\n",
      " [ 0.01442582  0.00961329  0.05994665  0.10669871  0.00513046  0.00296299\n",
      "   0.76495713  0.00624597  0.00628196  0.02373706]\n",
      " [ 0.76381767  0.053273    0.00021207  0.00402715  0.00222949  0.00170959\n",
      "   0.00341275  0.11228     0.0145471   0.04449115]\n",
      " [ 0.03391368  0.00339321  0.03119957  0.02570921  0.01568156  0.76457149\n",
      "   0.09773249  0.02635647  0.00112426  0.000318  ]\n",
      " [ 0.0928386   0.00714435  0.04251016  0.00682729  0.44294587  0.36128455\n",
      "   0.00086239  0.00201865  0.04289861  0.00066954]\n",
      " [ 0.10691828  0.11522738  0.05123224  0.06790637  0.05164959  0.2917946\n",
      "   0.06231887  0.15337919  0.04826916  0.05130436]\n",
      " [ 0.00037132  0.00550062  0.01627341  0.92806602  0.00053278  0.00011841\n",
      "   0.04369835  0.00026286  0.00235343  0.00282285]\n",
      " [ 0.00626737  0.00573534  0.62803376  0.01333638  0.07357697  0.0222723\n",
      "   0.21816416  0.01493411  0.01080949  0.00687018]\n",
      " [ 0.09745976  0.25370067  0.00836759  0.16323443  0.04131807  0.03317801\n",
      "   0.06607001  0.29650414  0.02454888  0.01561841]\n",
      " [ 0.00168242  0.0054481   0.83979392  0.02653616  0.03851262  0.00019734\n",
      "   0.03999237  0.00015408  0.04610942  0.00157365]\n",
      " [ 0.44538853  0.14218964  0.03434615  0.13434382  0.01057251  0.04743296\n",
      "   0.01653701  0.10730401  0.04235551  0.01952993]\n",
      " [ 0.03464424  0.25761566  0.00503909  0.01438555  0.59681726  0.01393892\n",
      "   0.01808284  0.02881901  0.02882948  0.00182795]\n",
      " [ 0.00011577  0.00384188  0.83469808  0.01141209  0.05215241  0.00763394\n",
      "   0.08708575  0.00227645  0.00070346  0.00008032]\n",
      " [ 0.00928567  0.00870975  0.03605982  0.01934253  0.00830943  0.00263199\n",
      "   0.89439034  0.01115867  0.00136161  0.00875008]\n",
      " [ 0.06474514  0.1566565   0.01705685  0.05272908  0.03569512  0.02783834\n",
      "   0.04317575  0.11711191  0.47100508  0.01398622]\n",
      " [ 0.07397913  0.05077309  0.01774839  0.0020973   0.06222759  0.78051364\n",
      "   0.0006698   0.00798245  0.00352027  0.00048826]\n",
      " [ 0.00062519  0.00036713  0.01338203  0.00208994  0.00041282  0.00001809\n",
      "   0.98271275  0.00025977  0.0000258   0.0001065 ]\n",
      " [ 0.26384163  0.08597461  0.01589226  0.04722494  0.08701253  0.10201161\n",
      "   0.03641645  0.12575312  0.10754319  0.12832971]\n",
      " [ 0.00001236  0.00003865  0.9687205   0.00090576  0.00526154  0.00094353\n",
      "   0.02398299  0.00001375  0.00008953  0.00003138]\n",
      " [ 0.09515239  0.15193565  0.01032504  0.47931463  0.00561017  0.01445285\n",
      "   0.04590408  0.09629869  0.04948595  0.05152055]\n",
      " [ 0.2435125   0.10524474  0.00111774  0.00597373  0.0781763   0.15685354\n",
      "   0.00536936  0.35261607  0.0417062   0.00942984]\n",
      " [ 0.10808641  0.02692148  0.03438781  0.0169737   0.06920635  0.02922697\n",
      "   0.47168779  0.00768961  0.15514593  0.08067387]\n",
      " [ 0.03519376  0.06654075  0.49021861  0.01383612  0.21388617  0.00036604\n",
      "   0.00520519  0.00287799  0.161009    0.01086642]\n",
      " [ 0.00014336  0.00064485  0.00655972  0.00618834  0.00068646  0.00003107\n",
      "   0.98509908  0.00046496  0.00003821  0.00014401]\n",
      " [ 0.0000487   0.00210271  0.64624208  0.0006745   0.31281027  0.03313547\n",
      "   0.00447972  0.00008006  0.00041914  0.00000742]\n",
      " [ 0.62421268  0.1416983   0.00140844  0.01962898  0.01168484  0.00400032\n",
      "   0.02331555  0.03537326  0.04966528  0.08901245]\n",
      " [ 0.00139272  0.03004543  0.00849898  0.02287604  0.26111197  0.61030895\n",
      "   0.00288225  0.05980677  0.00279624  0.00028068]\n",
      " [ 0.00056131  0.00269659  0.31805855  0.03203982  0.01176691  0.01195391\n",
      "   0.6207664   0.00119725  0.00021782  0.00074147]\n",
      " [ 0.00611554  0.02522279  0.02734473  0.00276376  0.91648918  0.00845289\n",
      "   0.00050854  0.00835789  0.00383804  0.00090665]\n",
      " [ 0.10705651  0.31505221  0.00990308  0.13132595  0.06316026  0.02960153\n",
      "   0.04123824  0.23744205  0.04267881  0.02254135]\n",
      " [ 0.00836535  0.01037808  0.01207376  0.0098278   0.139305    0.62805867\n",
      "   0.02516701  0.04186065  0.12346599  0.00149767]\n",
      " [ 0.00003053  0.00014443  0.92341477  0.00364382  0.04992035  0.00948168\n",
      "   0.01313186  0.00004209  0.00018394  0.0000066 ]\n",
      " [ 0.00003458  0.00222007  0.02920856  0.96026695  0.00074493  0.00005307\n",
      "   0.00683972  0.00001782  0.00010523  0.00050902]\n",
      " [ 0.02771185  0.0283191   0.02761601  0.01705121  0.64826506  0.20425732\n",
      "   0.00763423  0.02772873  0.00949174  0.00192484]\n",
      " [ 0.00020642  0.01560477  0.03947359  0.00117189  0.91212845  0.0100368\n",
      "   0.01683909  0.0018778   0.00261072  0.0000505 ]\n",
      " [ 0.13773827  0.07912122  0.0987058   0.01700571  0.33568653  0.02454602\n",
      "   0.0860847   0.02370656  0.11516037  0.08224475]\n",
      " [ 0.00029047  0.01294916  0.13316089  0.00592172  0.81674135  0.01659662\n",
      "   0.00069763  0.00060865  0.01301826  0.00001536]\n",
      " [ 0.19884332  0.02229027  0.06778901  0.00218931  0.2385634   0.00184039\n",
      "   0.02654492  0.00290562  0.41115418  0.02787958]\n",
      " [ 0.08676804  0.07488616  0.03413831  0.15984824  0.01966524  0.02574081\n",
      "   0.19168495  0.2856445   0.00770136  0.11392243]\n",
      " [ 0.0174343   0.05544876  0.10048044  0.09316386  0.00109713  0.00040514\n",
      "   0.72310013  0.00301171  0.00086539  0.00499317]\n",
      " [ 0.02782744  0.11275514  0.0481315   0.12950262  0.03426277  0.00326507\n",
      "   0.0657765   0.07713167  0.02153792  0.47980931]\n",
      " [ 0.33521667  0.13056877  0.04684719  0.04855559  0.00972513  0.00638851\n",
      "   0.28284165  0.12653002  0.00451185  0.00881465]\n",
      " [ 0.00284024  0.0298739   0.05144938  0.84369135  0.0048405   0.00058958\n",
      "   0.05717506  0.00021456  0.00154403  0.00778139]\n",
      " [ 0.00033206  0.00178477  0.00222882  0.00029458  0.17745565  0.81411952\n",
      "   0.00024621  0.00337804  0.00015471  0.00000561]\n",
      " [ 0.07254253  0.01842393  0.00218939  0.00308232  0.00703037  0.19139279\n",
      "   0.00250748  0.02831565  0.37954095  0.2949746 ]\n",
      " [ 0.09110466  0.05915476  0.05780754  0.23718099  0.01855827  0.00253347\n",
      "   0.20784301  0.00707782  0.31085533  0.0078841 ]\n",
      " [ 0.00158935  0.03315664  0.0196347   0.00499871  0.81787848  0.09334631\n",
      "   0.00144073  0.00175818  0.02601313  0.00018384]\n",
      " [ 0.33926475  0.03932365  0.00805004  0.0412566   0.00204603  0.00392594\n",
      "   0.42466161  0.10167198  0.00505225  0.03474724]\n",
      " [ 0.01180831  0.14050418  0.05172653  0.02632214  0.13244334  0.3756403\n",
      "   0.0335914   0.02418178  0.17510019  0.02868175]\n",
      " [ 0.00004308  0.00114021  0.95169133  0.00334387  0.01693199  0.00390426\n",
      "   0.02190199  0.00025801  0.00052008  0.0002652 ]\n",
      " [ 0.59783477  0.00535313  0.00041308  0.02021066  0.0004186   0.00036316\n",
      "   0.0297632   0.33809814  0.00111483  0.00643037]\n",
      " [ 0.01411336  0.02704371  0.00102967  0.10913639  0.00011482  0.00020991\n",
      "   0.0010925   0.01890449  0.00167547  0.82667965]\n",
      " [ 0.00246772  0.02927997  0.03802498  0.63714916  0.00251128  0.00070715\n",
      "   0.28648493  0.00144889  0.00035281  0.00157318]\n",
      " [ 0.00382224  0.00682176  0.0009986   0.00024894  0.00652643  0.00179104\n",
      "   0.00053496  0.00043122  0.9679212   0.01090371]\n",
      " [ 0.04998269  0.03989587  0.01485968  0.50068814  0.00572271  0.03227625\n",
      "   0.01587832  0.01825526  0.0367722   0.28566894]\n",
      " [ 0.02229078  0.00317713  0.00002767  0.00061921  0.00004002  0.00000219\n",
      "   0.00120643  0.00113326  0.00503424  0.96646911]\n",
      " [ 0.00620961  0.11481471  0.1543577   0.03735008  0.54976887  0.0110674\n",
      "   0.01373261  0.00562247  0.09750131  0.00957509]\n",
      " [ 0.16573814  0.09399985  0.03580298  0.01602932  0.12264503  0.1765725\n",
      "   0.02423899  0.09094476  0.17781079  0.09621756]\n",
      " [ 0.03184604  0.02242504  0.00386794  0.00257826  0.03371467  0.00204665\n",
      "   0.00699407  0.00402767  0.74295986  0.14953977]\n",
      " [ 0.39894214  0.2471754   0.00748502  0.01121647  0.02898425  0.00211796\n",
      "   0.08193561  0.03935745  0.1518326   0.03095308]\n",
      " [ 0.02340378  0.00165042  0.00001393  0.00051821  0.00008315  0.00000393\n",
      "   0.00103966  0.00266672  0.01638353  0.95423669]\n",
      " [ 0.04947165  0.09847494  0.00207708  0.01178678  0.03509645  0.01279344\n",
      "   0.00581065  0.67471176  0.10818711  0.00159014]\n",
      " [ 0.00055323  0.00280199  0.00113583  0.00044234  0.96550727  0.02619076\n",
      "   0.00037169  0.00071358  0.00226094  0.00002238]\n",
      " [ 0.03565009  0.02346472  0.36146998  0.09238418  0.19653739  0.02020667\n",
      "   0.16789483  0.01326391  0.04906998  0.04005833]\n",
      " [ 0.00016255  0.00282459  0.01124152  0.00081711  0.73531348  0.24158438\n",
      "   0.00153818  0.00115215  0.0053284   0.00003774]\n",
      " [ 0.8883636   0.03880421  0.00004037  0.00358384  0.00183475  0.00052616\n",
      "   0.00208545  0.04322906  0.00443846  0.01709398]\n",
      " [ 0.00444092  0.06185902  0.04773075  0.00361507  0.7865693   0.0641603\n",
      "   0.00244608  0.00714778  0.02087921  0.00115161]\n",
      " [ 0.15796393  0.05055753  0.03270467  0.03978804  0.15855704  0.47441474\n",
      "   0.00513412  0.01533035  0.05225072  0.0132989 ]\n",
      " [ 0.00829894  0.01236476  0.02971615  0.05843385  0.00026965  0.00020724\n",
      "   0.88169783  0.00302398  0.00395764  0.00202993]\n",
      " [ 0.00330346  0.684605    0.00285652  0.02873141  0.21027438  0.0067732\n",
      "   0.00628133  0.05417624  0.00181112  0.00118737]\n",
      " [ 0.01365716  0.56872344  0.0021504   0.0100118   0.08765992  0.00321646\n",
      "   0.01817294  0.29276621  0.00237443  0.00126731]\n",
      " [ 0.98423058  0.0036982   0.00014288  0.00008601  0.00034012  0.00014831\n",
      "   0.00060738  0.00458413  0.00164308  0.00451925]\n",
      " [ 0.00192827  0.0041654   0.0087784   0.00058705  0.16831882  0.80668348\n",
      "   0.00173686  0.00735585  0.00041079  0.00003517]\n",
      " [ 0.98462093  0.00096782  0.00003874  0.00022417  0.00148752  0.00035357\n",
      "   0.00096376  0.0039862   0.00690924  0.00044794]\n",
      " [ 0.00001095  0.00021515  0.97484595  0.00205356  0.01234121  0.00326657\n",
      "   0.00713928  0.00004093  0.00005588  0.00003051]\n",
      " [ 0.00090774  0.00285595  0.01869573  0.0032366   0.14606236  0.77519518\n",
      "   0.00260775  0.00113673  0.04839382  0.00090818]\n",
      " [ 0.00364573  0.06180684  0.02887324  0.02402305  0.02184289  0.00403863\n",
      "   0.01731207  0.0262903   0.79219562  0.0199716 ]\n",
      " [ 0.02273341  0.09666131  0.28968734  0.13967529  0.26793155  0.01447198\n",
      "   0.07887759  0.01184565  0.03377477  0.04434109]\n",
      " [ 0.00121548  0.01493636  0.01757981  0.00694623  0.00428941  0.00042259\n",
      "   0.95079255  0.0003844   0.00055876  0.00287457]\n",
      " [ 0.02391022  0.04783408  0.0084388   0.07933468  0.03095151  0.00181481\n",
      "   0.0479553   0.02210299  0.72445089  0.01320673]\n",
      " [ 0.00897631  0.01286715  0.04167316  0.74633384  0.00101891  0.00063808\n",
      "   0.1635856   0.02004835  0.00066047  0.00419831]\n",
      " [ 0.00090783  0.00048923  0.0000013   0.00163811  0.00000129  0.00000014\n",
      "   0.00007729  0.00010475  0.0001626   0.9966175 ]\n",
      " [ 0.01414756  0.00212431  0.00048446  0.00119028  0.00199325  0.02104759\n",
      "   0.00105443  0.0005217   0.88418239  0.0732541 ]\n",
      " [ 0.00102658  0.0228674   0.03439218  0.82591158  0.00396412  0.00034578\n",
      "   0.10228021  0.00112689  0.00051521  0.00757003]\n",
      " [ 0.97340775  0.0046666   0.00056586  0.00072742  0.00540223  0.00069086\n",
      "   0.00257566  0.00132057  0.01026447  0.00037859]\n",
      " [ 0.12507069  0.18813388  0.01324484  0.02698804  0.06861699  0.05737413\n",
      "   0.08092204  0.08504105  0.10593522  0.2486731 ]\n",
      " [ 0.93514937  0.02198785  0.00007879  0.00157247  0.00047143  0.00023528\n",
      "   0.00095265  0.01867343  0.00094864  0.01993009]\n",
      " [ 0.00869762  0.00498244  0.00033765  0.00133599  0.00228437  0.00200932\n",
      "   0.00100503  0.00059959  0.16672686  0.81202108]\n",
      " [ 0.00190046  0.00092034  0.00000597  0.00190229  0.00000636  0.00000235\n",
      "   0.00127833  0.00055475  0.00093705  0.99249214]\n",
      " [ 0.00096137  0.29303533  0.00422913  0.00147433  0.66516578  0.01039841\n",
      "   0.00057346  0.02019675  0.00379206  0.00017341]\n",
      " [ 0.00380579  0.00588105  0.00418004  0.00210224  0.14424317  0.81196445\n",
      "   0.00576356  0.02088774  0.00115195  0.00001996]\n",
      " [ 0.00823145  0.16215771  0.20673379  0.00357094  0.4594329   0.02386311\n",
      "   0.07870482  0.0365339   0.01666424  0.00410708]\n",
      " [ 0.059052    0.03200526  0.04716923  0.37684262  0.00120136  0.01058232\n",
      "   0.39056513  0.01931093  0.00986834  0.05340285]\n",
      " [ 0.06524175  0.07488406  0.00011192  0.00328101  0.00668964  0.00162206\n",
      "   0.00071127  0.84662384  0.0002916   0.00054285]\n",
      " [ 0.00038079  0.03542101  0.00498514  0.00227148  0.92407453  0.02765938\n",
      "   0.00068528  0.00294605  0.00150169  0.00007463]\n",
      " [ 0.00187842  0.00669888  0.08060286  0.08760062  0.00928578  0.00023226\n",
      "   0.80973434  0.0036312   0.00010047  0.00023518]] (20.636 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 2900 into /tmp/notmnist_convnet_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.854049.\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-05-19:44:03\n",
      "INFO:tensorflow:Restoring parameters from /tmp/notmnist_convnet_model/model.ckpt-2900\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-05-19:44:10\n",
      "INFO:tensorflow:Saving dict for global step 2900: accuracy = 0.7896, global_step = 2900, loss = 0.800827\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "{'global_step': 2900, 'loss': 0.80082685, 'accuracy': 0.78960001}\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rgparekh/Applications/miniconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2855: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# Our application logic will be added here\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "  \"\"\"Model function for CNN.\"\"\"\n",
    "  # Input Layer\n",
    "  # batch_size = -1 indicates a place holder to dynamically fill in the batch_size\n",
    "  input_layer = tf.reshape(features[\"x\"], [-1, image_size, image_size, 1])\n",
    "\n",
    "  # Convolutional Layer #1\n",
    "  conv1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=32,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "  # Pooling Layer #1\n",
    "  pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "  # Convolutional Layer #2 and Pooling Layer #2\n",
    "  conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=64,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "  pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "  # Dense Layer\n",
    "  pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "  dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "  dropout = tf.layers.dropout(\n",
    "      inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "  # Logits Layer\n",
    "  logits = tf.layers.dense(inputs=dropout, units=num_labels)\n",
    "\n",
    "  predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "  }\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "  # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "  onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)\n",
    "  loss = tf.losses.softmax_cross_entropy(onehot_labels=onehot_labels, logits=logits)\n",
    "\n",
    "  # Configure the Training Op (for TRAIN mode)\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    train_op = optimizer.minimize(\n",
    "        loss=loss,\n",
    "        global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "  # Add evaluation metrics (for EVAL mode)\n",
    "  eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions[\"classes\"])}\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(unused_argv):\n",
    "    # Load the pickle file\n",
    "    pickle_file = '/Users/rgparekh/Documents/Personal/Rajesh/Data/notMNIST.pickle'\n",
    "\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "      save = pickle.load(f)\n",
    "      train_dataset = save['train_dataset']\n",
    "      train_labels = save['train_labels']\n",
    "      valid_dataset = save['valid_dataset']\n",
    "      valid_labels = save['valid_labels']\n",
    "      test_dataset = save['test_dataset']\n",
    "      test_labels = save['test_labels']\n",
    "      del save  # hint to help gc free up memory\n",
    "      print('Training set', train_dataset.shape, train_labels.shape)\n",
    "      print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "      print('Test set', test_dataset.shape, test_labels.shape)\n",
    "    \n",
    "    # Reformat the datasets\n",
    "    #train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "    #valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "    #test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "    #print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    #print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    #print('Test set', test_dataset.shape, test_labels.shape)\n",
    "    \n",
    "    classifier = tf.estimator.Estimator(model_fn=cnn_model_fn, model_dir=\"/tmp/notmnist_convnet_model\")\n",
    "    \n",
    "    # Set up logging for predictions\n",
    "    tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "    logging_hook = tf.train.LoggingTensorHook(\n",
    "        tensors=tensors_to_log, every_n_iter=100)\n",
    "    \n",
    "    # Train the model\n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": train_dataset},\n",
    "        y=train_labels,\n",
    "        batch_size=100,\n",
    "        num_epochs=None,\n",
    "        shuffle=True)\n",
    "    classifier.train(\n",
    "        input_fn=train_input_fn,\n",
    "        steps=500,\n",
    "        hooks=[logging_hook])\n",
    "    \n",
    "    # Evaluate the model and print results\n",
    "    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": valid_dataset},\n",
    "        y=valid_labels,\n",
    "        num_epochs=1,\n",
    "        shuffle=False)\n",
    "    eval_results = classifier.evaluate(input_fn=eval_input_fn)\n",
    "    print(eval_results)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1  [256, 14, 14, 16]\n",
      "Hidden1  [256, 14, 14, 16]\n",
      "Pool1  [256, 7, 7, 16]\n",
      "Conv2  [256, 4, 4, 32]\n",
      "Hidden2  [256, 4, 4, 32]\n",
      "Pool2  [256, 2, 2, 32]\n",
      "Conv1  [10000, 14, 14, 16]\n",
      "Hidden1  [10000, 14, 14, 16]\n",
      "Pool1  [10000, 7, 7, 16]\n",
      "Conv2  [10000, 4, 4, 32]\n",
      "Hidden2  [10000, 4, 4, 32]\n",
      "Pool2  [10000, 2, 2, 32]\n",
      "Conv1  [10000, 14, 14, 16]\n",
      "Hidden1  [10000, 14, 14, 16]\n",
      "Pool1  [10000, 7, 7, 16]\n",
      "Conv2  [10000, 4, 4, 32]\n",
      "Hidden2  [10000, 4, 4, 32]\n",
      "Pool2  [10000, 2, 2, 32]\n"
     ]
    }
   ],
   "source": [
    "# CNN architecture with max-pooling\n",
    "\n",
    "# Define the CNN architecture\n",
    "\n",
    "batch_size = 256\n",
    "patch_size = 5\n",
    "depth1 = 16\n",
    "depth2 = 32\n",
    "num_hidden = 64\n",
    "num_channels = 1\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth1], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth1]))\n",
    "\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth1, depth2], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth2]))\n",
    "\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 14 * image_size // 14 * depth2, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    # data is the input data\n",
    "    # layer1_weights define the filter parameter [filter_height, filter_width, input_channels, output_channels]\n",
    "    # [1,2,2,1] defines the strides parameter - 1D tensor of length 4 (batch, height, width, channels)\n",
    "    conv1 = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    # conv1 returns a tensor of size [batch_size, image_size/2, image_size/2, depth1]\n",
    "    \n",
    "    hidden1 = tf.nn.relu(conv1 + layer1_biases)\n",
    "    \n",
    "    # hidden1 returns a tensor of size [batch_size, image_size/2, image_size/2, depth1]\n",
    "    \n",
    "    pool1 = tf.nn.max_pool(value=hidden1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    \n",
    "    # pool1 returns a tensor of size [batch_size, image_size/4, image_size/4, depth1] (dim reduction since strides = 2)    \n",
    "    \n",
    "    #print(\"Conv1 \", conv1.get_shape().as_list())\n",
    "    #print(\"Hidden1 \", hidden1.get_shape().as_list())\n",
    "    #print(\"Pool1 \", pool1.get_shape().as_list())\n",
    "    \n",
    "    conv2 = tf.nn.conv2d(pool1, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "   # conv1 returns a tensor of size [batch_size, image_size/4/2, image_size/4/2, depth2]\n",
    "\n",
    "    hidden2 = tf.nn.relu(conv2 + layer2_biases)\n",
    "    \n",
    "  # hidden2 returns a tensor of size [batch_size, image_size/8, image_size/8, depth2]\n",
    "\n",
    "    pool2 = tf.nn.max_pool(value=hidden2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    \n",
    "  # pool2 returns a tensor of size [batch_size, image_size/14, image_size/14, depth2]\n",
    "    \n",
    "    print(\"Conv2 \", conv2.get_shape().as_list())    \n",
    "    print(\"Hidden2 \", hidden2.get_shape().as_list())    \n",
    "    print(\"Pool2 \", pool2.get_shape().as_list())\n",
    "    \n",
    "    shape = pool2.get_shape().as_list()\n",
    "    reshape = tf.reshape(pool2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    \n",
    "    hidden3 = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    \n",
    "    return tf.matmul(hidden3, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.AdamOptimizer(0.005).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train model and compute test set accuracy\n",
    "\n",
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
